Here's an example of a multi-class classifier model including pre-processing and inference pipeline using best practices and architectural guidelines from the provided link:

```python
import torch
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset, DataLoader
import pandas as pd

# Load dataset
df = pd.read_csv('your_dataset.csv')

# Preprocess data
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
max_len = 512
scaler = StandardScaler()

def preprocess_data(data):
    inputs = tokenizer.encode_plus(
        data['text'],
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    labels = data['label']
    return {
        'input_ids': inputs['input_ids'].flatten(),
        'attention_mask': inputs['attention_mask'].flatten(),
        'labels': torch.tensor(labels)
    }

# Split dataset into training and validation sets
train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)

# Create custom dataset class for our data
class YourDataset(Dataset):
    def __init__(self, data, tokenizer, max_len):
        self.data = data
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __getitem__(self, idx):
        input_ids = self.tokenizer.encode_plus(
            self.data.iloc[idx, 0],
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        labels = torch.tensor(self.data.iloc[idx, -1])
        return {
            'input_ids': input_ids['input_ids'].flatten(),
            'attention_mask': input_ids['attention_mask'].flatten(),
            'labels': labels
        }

    def __len__(self):
        return len(self.data)

# Create dataset and data loader instances
train_dataset = YourDataset(train_data, tokenizer, max_len)
val_dataset = YourDataset(val_data, tokenizer, max_len)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)

# Define model and optimizer
class YourModel(torch.nn.Module):
    def __init__(self):
        super(YourModel, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = torch.nn.Dropout(p=0.1)
        self.classifier = torch.nn.Linear(self.bert.config.hidden_size, 8)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        outputs = self.classifier(pooled_output)
        return outputs

model = YourModel()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)

# Train model
for epoch in range(5):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = torch.nn.CrossEntropyLoss()(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}')

# Evaluate model
model.eval()
with torch.no_grad():
    total_correct = 0
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask)
        _, predicted = torch.max(outputs, dim=1)
        total_correct += (predicted == labels).sum().item()
    accuracy = total_correct / len(val_data)
    print(f'Validation Accuracy: {accuracy:.4f}')

# Use the trained model for inference
def make_prediction(text):
    input_ids = tokenizer.encode_plus(
        text,
        add_special_tokens=True,
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    with torch.no_grad():
        outputs = model(input_ids['input_ids'].to(device), input_ids['attention_mask'].to(device))
        _, predicted = torch.max(outputs, dim=1)
        return predicted.item()

# Example usage
text = 'Your text here'
prediction = make_prediction(text)
print(f'Prediction: {prediction}')
```

This code defines a custom dataset class for our data, creates a dataset and data loader instances, defines a model and optimizer, trains the model, evaluates its performance on the validation set, and uses it for inference. The model is trained using the Adam optimizer with a learning rate of 1e-5, and the batch size is set to 32. The model's accuracy is evaluated on the validation set after each epoch, and the final prediction is made by passing the input text through the model.